{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Case Study 4.1 - Movies"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h1 style=\"color:red;\">Note: If you close this notebook at any time, you will have to run all cells again upon re-opening it.</h1>\n\n<h1 style=\"color:red;\">Note: You may get different numerical results running the notebook different times. This is to be expected, you can just report whatever results you get.</h1>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# ADVANCED PYTHON"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As this is an advanced version, we don't include a lot of code here. If you get stuck on a particular part, feel free to also use the beginner version in `beginnner_python.ipynb` to help you out."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Identification Information"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# YOUR NAME              = ...\n# YOUR MITX PRO USERNAME = ...\n# YOUR MITX PRO E-MAIL   = ...",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Setup"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Run these cells to install all the packages you need to complete the remainder of the case study. This may take a few minutes, so please be patient."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade pip\n!pip install surprise==0.1",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting pip\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/36/67f809c135c17ec9b8276466cc57f35b98c240f55c780689ea29fa32f512/pip-20.0.1-py2.py3-none-any.whl (1.5MB)\n\u001b[K     |████████████████████████████████| 1.5MB 165kB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pip\n  Found existing installation: pip 19.3.1\n    Uninstalling pip-19.3.1:\n      Successfully uninstalled pip-19.3.1\nSuccessfully installed pip-20.0.1\nCollecting surprise==0.1\n  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\nCollecting scikit-surprise\n  Downloading scikit-surprise-1.1.0.tar.gz (6.4 MB)\n\u001b[K     |████████████████████████████████| 6.4 MB 257 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: joblib>=0.11 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-surprise->surprise==0.1) (0.14.0)\nRequirement already satisfied: numpy>=1.11.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-surprise->surprise==0.1) (1.16.2)\nRequirement already satisfied: scipy>=1.0.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-surprise->surprise==0.1) (1.1.0)\nRequirement already satisfied: six>=1.10.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-surprise->surprise==0.1) (1.11.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.0-cp36-cp36m-linux_x86_64.whl size=1713349 sha256=b6a5bc56c01d406022eedbb66325f920630e8415eea00f5cd8c33a45a46f205e\n  Stored in directory: /home/nbuser/.cache/pip/wheels/f3/97/d9/6a242e5e3bb89377d4f575af72d14af0d54ebd90a525a4c2f9\nSuccessfully built scikit-surprise\nInstalling collected packages: scikit-surprise, surprise\nSuccessfully installed scikit-surprise-1.1.0 surprise-0.1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, you must press **Kernel > Restart.** This allows the installation to take effect. Once you see the blue **Connected/Kernel ready** button in the top right, you are good to go."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Import"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport matplotlib\nfrom surprise import Dataset, SVD, NormalPredictor, BaselineOnly, KNNBasic, NMF\nfrom surprise.model_selection import cross_validate, KFold",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Use the [`**Dataset.load_builtin**`](http://surprise.readthedocs.io/en/stable/dataset.html#surprise.dataset.Dataset.load_builtin) function to load the data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Your code here to load the data...",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We also want to get a sense of what the data looks like. Please create a histogram of all the ratings we have in the dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Your code here to create a ratings histogram...",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h1 style=\"color:red;\">QUESTION 1: DATA ANALYSIS</h1>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Describe the dataset. How many ratings are in the dataset? How would you describe the distribution of ratings? Is there anything else we should observe? Make sure the histogram is visible in the notebook.**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Type your response here...*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Model 1: Random"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create model using NormalPredictor() class",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train on data using cross-validation with k=5 folds, measuring the RMSE\n# See the cross_validate function that we have imported above\n# http://surprise.readthedocs.io/en/stable/model_selection.html#surprise.model_selection.validation.cross_validate",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Model 2: User-Based Collaborative Filtering"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create model using KNNBasic() class\n# See the sim_options parameter to determine the user/item similarity calculation of the model\n# http://surprise.readthedocs.io/en/stable/prediction_algorithms.html#similarity-measures-configuration",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train using same cross validation code as above",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Model 3: Item-Based Collaborative Filtering"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create model using KNNBasic() class\n# Make sure you change the sim_options parameter from above",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train using same cross validation code as above",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h1 style=\"color:red;\">QUESTION 2: COLLABORATIVE FILTERING MODELS</h1>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Compare the results from the user-user and item-item models. How do they compare to each other? How do they compare to our original \"random\" model? Can you provide any intuition as to why the results came out the way they did?**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Type your response here...*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Model 4: Matrix Factorization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create model using SVD() class",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train using same cross validation code as above",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h1 style=\"color:red;\">QUESTION 3: MATRIX FACTORIZATION MODEL</h1>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**The matrix factorization model is different from the collaborative filtering models. Briefly describe this difference. Also, compare the RMSE again. Does it improve? Can you offer any reasoning as to why that might be?**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Type your response here...*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Precision and Recall @ `k`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now want to compute the precision and recall for 2 values of `k`: 5 and 10. We have provided some code here to help you do that."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, we define a function that takes in some predictions, a value of `k` and a threshold parameter. This code is adapted from [here](http://surprise.readthedocs.io/en/stable/FAQ.html?highlight=precision#how-to-compute-precision-k-and-recall-k)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n    '''Return precision and recall at k metrics for each user.'''\n\n    # First map the predictions to each user.\n    user_est_true = dict()\n    for uid, _, true_r, est, _ in predictions:\n        current = user_est_true.get(uid, list())\n        current.append((est, true_r))\n        user_est_true[uid] = current\n\n    precisions = dict()\n    recalls = dict()\n    for uid, user_ratings in user_est_true.items():\n\n        # Sort user ratings by estimated value\n        user_ratings.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n\n        # Recall@K: Proportion of relevant items that are recommended\n        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n\n    return precisions, recalls",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we compute the precision and recall at `k` = 5 and 10 for each of our 4 models. We use 5-fold cross validation again to average the results across the entire dataseat.\n\nPlease note that this will take some time to compute."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h1 style=\"color:red;\">QUESTION 4: PRECISION/RECALL</h1>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Compute the precision and recall, for each of the 4 models, at `k` = 5 and 10. This is 2 x 2 x 4 = 16 numerical values. Do you note anything interesting about these values? Anything differerent from the RMSE values you computed above?**\n\nSome code is required for this question."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Use the function above to compute the 16 numerical values requested above\n# See the test() function to get the predictions input to the function\n# http://surprise.readthedocs.io/en/stable/algobase.html#surprise.prediction_algorithms.algo_base.AlgoBase.test",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Type your response here...*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#  Top-`n` Predictions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, we can see what some of the actual movie ratings are for particular users, as outputs of our model."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, we define a helpful function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_top_n(predictions, n=5):\n    '''Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    '''\n\n    # First map the predictions to each user.\n    top_n = dict()\n    for uid, iid, true_r, est, _ in predictions:\n        current = top_n.get(uid, [])\n        current.append((iid, est))\n        top_n[uid] = current\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Then, we call this function on each of our models, first training on **all** the data we have available, then predicting on the remaining, missing data. We use `n`=5 here, but you can pick any reasonable value of `n` you would like.\n\nThis may take some time to compute, so be patient."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Hint: Use [`**Dataset.build_full_trainset**`](http://surprise.readthedocs.io/en/stable/dataset.html#surprise.dataset.DatasetAutoFolds.build_full_trainset) to get the full trainset from the data. Then call [`**Trainset.build_anti_testset**`](http://surprise.readthedocs.io/en/stable/trainset.html#surprise.Trainset.build_anti_testset) to get the testset out. Finally, `fit` on the trainset, `test` on the testset, then pass that result to our `get_top_n` function."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h1 style=\"color:red;\">QUESTION 5: TOP N PREDICTIONS</h1>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Do the top n predictions that you received make sense? What is the rating value (1-5) of these predictions? How could you use these predictions in the real-world if you were trying to build a generic content recommender system for a company?**\n\nSome code is required for this question."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Use the function and hints above to give the top-n predictions for a given user, for a reasonable value of n",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Type your response here...*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Great job! Now, make sure you check out the **Conclusion** section of the [instruction manual](https://courses.edx.org/asset-v1:MITxPRO+DSx+2T2018+type@asset+block@4.1_instruction_manual.html) to wrap up this case study properly."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}